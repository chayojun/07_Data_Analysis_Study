{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9904241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc23aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40725dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4915af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "816318c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# topic 1:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'arfsort'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, topic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lda\u001b[38;5;241m.\u001b[39mcomponents_):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# topic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m     topic_word_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtopic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marfsort\u001b[49m()[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     55\u001b[0m     top_idxs \u001b[38;5;241m=\u001b[39m topic_word_idx[:\u001b[38;5;241m20\u001b[39m]\n\u001b[0;32m     56\u001b[0m     topics \u001b[38;5;241m=\u001b[39m [feature_names[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m top_idxs]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'arfsort'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"7월_프로젝트_데이터/구글플레이스토어_서울페이_리뷰.csv\", index_col=0)\n",
    "\n",
    "# 불러온 데이터를 전처리 후 형태소 분석기를 돌리기\n",
    "review_list = []\n",
    "for i, review in enumerate(data[\"content\"]):\n",
    "    # Step1. 정규표현식 전처리하기\n",
    "    review = re.sub(\"[^a-zA-Z0-9가-힣\\s]\", \"\", review)\n",
    "    # Step2. 형태소 분석기\n",
    "    result = okt.morphs(review)\n",
    "    # Step3. 담기\n",
    "    # print(f\"{i}번째 리뷰: {result}\")\n",
    "    # print(f\"{i}번째 리뷰: {' '.join(result)}\")\n",
    "    # word_list.append(result)\n",
    "    review_list.append(\" \".join(result))\n",
    "\n",
    "\n",
    "\n",
    "# 전체 단어 등장 비율이 p 이상인 것만 사용할 것.\n",
    "# 단어가 적어도 n개 이상인 것만 사용할 것.\n",
    "# 상위 항목 n개만 사용할 것.\n",
    "# 단어의 조합\n",
    "vectorizer = CountVectorizer(\n",
    "    max_df = 0.1,               \n",
    "    min_df = 2,                 \n",
    "    max_features = 1000,        \n",
    "    ngram_range = (1,2)         \n",
    ")\n",
    "\n",
    "\n",
    "feature_vec = vectorizer.fit_transform(review_list)\n",
    "# feature_vec.shape\n",
    "\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# print(feature_names)\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "lda.fit(feature_vec)\n",
    "\n",
    "\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"# topic {idx+1}:\")\n",
    "    topic_word_idx = topic.arfsort()[::-1]\n",
    "    top_idxs = topic_word_idx[:20]\n",
    "    topics = [feature_names[idx] for idx in top_idxs]\n",
    "    print(\" \".join(topics))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "POTENUP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
